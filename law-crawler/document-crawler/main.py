

import pandas as pd
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.dialects.mysql import LONGTEXT
from sqlalchemy.ext.declarative import declarative_base
import re
from bs4 import BeautifulSoup
import requests

# T·∫°o k·∫øt n·ªëi v·ªõi c∆° s·ªü d·ªØ li·ªáu
engine = create_engine("mysql+mysqlconnector://root:123456@localhost:3306/law")

# ƒê·ªçc d·ªØ li·ªáu t·ª´ c∆° s·ªü d·ªØ li·ªáu
df = pd.read_sql('SELECT vbqppl_link FROM pddieu GROUP BY vbqppl_link;', con=engine)

# Kh·ªüi t·∫°o c∆° s·ªü d·ªØ li·ªáu v·ªõi SQLAlchemy
Base = declarative_base()

class VBPL(Base):
    __tablename__ = 'vbpl'

    id = Column(Integer, primary_key=True)
    noidung = Column(LONGTEXT, nullable=False)

# T·∫°o b·∫£ng n·∫øu ch∆∞a t·ªìn t·∫°i
Base.metadata.create_all(engine)

def get_infor(url):
    if url == None:
        return None
    match = re.search(r'ItemID=(\d+).*#(.*)', url)
    if match:
        item_id = match.group(1)
        return item_id
    else:
        print('Kh√¥ng t√¨m th·∫•y kh·ªõp.')

def save_data(list_id, list_noidung):
    # Ghi d·ªØ li·ªáu v√†o c∆° s·ªü d·ªØ li·ªáu t·ª´ DataFrame
    df_to_write = pd.DataFrame({
        'id': list_id,
        'noidung': list_noidung
    })
    df_to_write.to_sql('vbpl', con=engine, if_exists='append', index=False)

list_vb = [get_infor(df.iloc[i]['vbqppl_link']) for i in range(len(df))]

print(len(df))

df_vb = pd.DataFrame(list_vb)
# Lo·∫°i b·ªè c√°c gi√° tr·ªã None
df_vb = df_vb.dropna()
# Lo·∫°i b·ªè c√°c gi√° tr·ªã tr√πng nhau
df_vb = df_vb.drop_duplicates()

print(len(df_vb))
list_id = []
list_noidung = []
for i in range(len(df_vb)):
    id = df_vb.iloc[i][0]
    print(i, "Get data id ", id)
    url_content = f'https://vbpl.vn/TW/Pages/vbpq-toanvan.aspx?ItemID={id}'
    try:
        response = requests.get(url_content, timeout=3)
        soup = BeautifulSoup(response.content, 'html.parser')
        div_text = soup.find_all('div', class_='fulltext')[0]
        noidung = div_text.find_all('div')[1]
        
        # Lo·∫°i b·ªè c√°c th·∫ª HTML v√† l·∫•y vƒÉn b·∫£n thu·∫ßn t√∫y
        noidung_text = noidung.get_text(separator=' ', strip=True)
        
        list_id.append(id)
        list_noidung.append(noidung_text)
    except:
        continue

    if i % 10 == 0:
        save_data(list_id, list_noidung)
        list_id.clear()
        list_noidung.clear()
    print("Successfully fetched and saved data for ID:", id)

# Ghi d·ªØ li·ªáu cu·ªëi c√πng v√†o c∆° s·ªü d·ªØ li·ªáu
save_data(list_id, list_noidung)
print("Data saved successfully.")












# import pandas as pd
# from sqlalchemy import create_engine

# # K·∫øt n·ªëi DB
# engine = create_engine("mysql+mysqlconnector://root:123456@localhost:3306/law")

# # ƒê·ªçc danh s√°ch ID t·ª´ b·∫£ng pddieu
# df = pd.read_sql('SELECT vbqppl_link FROM pddieu GROUP BY vbqppl_link;', con=engine)

# # H√†m l·∫•y ID t·ª´ URL
# import re
# def get_infor(url):
#     if not isinstance(url, str):  # Ki·ªÉm tra n·∫øu url kh√¥ng ph·∫£i l√† chu·ªói
#         return None
#     match = re.search(r'ItemID=(\d+)', url)
#     return match.group(1) if match else None

# # L·∫•y danh s√°ch ID t·ª´ b·∫£ng pddieu
# df['id'] = df['vbqppl_link'].apply(get_infor)
# df_vb = df[['id']].dropna().drop_duplicates()

# # ƒê·ªçc danh s√°ch ID ƒë√£ l∆∞u trong b·∫£ng vbpl
# existing_ids = pd.read_sql('SELECT id FROM vbpl;', con=engine)

# # ƒê·∫øm s·ªë l∆∞·ª£ng ID
# count_fast = len(df_vb)  # ƒê·∫øm nhanh
# count_real = len(existing_ids)  # ƒê·∫øm th·ª±c t·∫ø ƒë√£ l∆∞u

# print(f"S·ªë ID ƒë·∫øm nhanh: {count_fast}")
# print(f"S·ªë ID ƒë√£ l∆∞u v√†o DB: {count_real}")

# # X√°c ƒë·ªãnh ID n√†o b·ªã thi·∫øu
# missing_ids = set(df_vb['id']) - set(existing_ids['id'])
# print(f"S·ªë ID b·ªã thi·∫øu: {len(missing_ids)}")

# # N·∫øu mu·ªën in danh s√°ch ID b·ªã thi·∫øu
# print(missing_ids)














# import pandas as pd
# from sqlalchemy import create_engine, Column, Integer
# from sqlalchemy.dialects.mysql import LONGTEXT
# from sqlalchemy.orm import declarative_base
# import re
# from bs4 import BeautifulSoup
# import requests
# import time

# # K·∫øt n·ªëi ƒë·∫øn c∆° s·ªü d·ªØ li·ªáu MySQL
# engine = create_engine("mysql+mysqlconnector://root:123456@localhost:3306/law")

# # Kh·ªüi t·∫°o c∆° s·ªü d·ªØ li·ªáu v·ªõi SQLAlchemy
# Base = declarative_base()

# class VBPL(Base):
#     __tablename__ = 'vbpl'

#     id = Column(Integer, primary_key=True)
#     noidung = Column(LONGTEXT, nullable=False)

# # T·∫°o b·∫£ng n·∫øu ch∆∞a t·ªìn t·∫°i
# Base.metadata.create_all(engine)

# def get_infor(url):
#     """Tr√≠ch xu·∫•t ID t·ª´ ƒë∆∞·ªùng link vbqppl_link"""
#     if url is None:
#         return None
#     match = re.search(r'ItemID=(\d+)', url)
#     if match:
#         return match.group(1)
#     return None

# def save_data(list_id, list_noidung):
#     """L∆∞u d·ªØ li·ªáu v√†o b·∫£ng vbpl"""
#     if not list_id:
#         return
#     df_to_write = pd.DataFrame({'id': list_id, 'noidung': list_noidung})
#     df_to_write.to_sql('vbpl', con=engine, if_exists='append', index=False)
#     print(f"‚úÖ ƒê√£ l∆∞u {len(list_id)} b·∫£n ghi v√†o DB.")

# # L·∫•y danh s√°ch t·∫•t c·∫£ ID t·ª´ b·∫£ng pddieu
# df = pd.read_sql('SELECT vbqppl_link FROM pddieu GROUP BY vbqppl_link;', con=engine)
# list_vb = [get_infor(link) for link in df['vbqppl_link']]
# df_vb = pd.DataFrame(list_vb, columns=['id']).dropna().drop_duplicates()

# # L·∫•y danh s√°ch ID ƒë√£ c√≥ trong b·∫£ng vbpl
# existing_ids = pd.read_sql('SELECT id FROM vbpl;', con=engine)['id'].astype(str).tolist()

# # L·ªçc ra c√°c ID c√≤n thi·∫øu
# missing_ids = df_vb[~df_vb['id'].astype(str).isin(existing_ids)]['id'].tolist()
# print(f"üìå S·ªë ID c·∫ßn t·∫£i l·∫°i: {len(missing_ids)}")

# # Crawl l·∫°i n·ªôi dung cho c√°c ID b·ªã thi·∫øu
# list_id, list_noidung = [], []

# for i, id in enumerate(missing_ids):
#     print(f"{i+1}/{len(missing_ids)} - Get data ID {id}")
#     url_content = f'https://vbpl.vn/TW/Pages/vbpq-toanvan.aspx?ItemID={id}'
    
#     retry_count = 0
#     success = False
#     while retry_count < 3 and not success:
#         try:
#             response = requests.get(url_content, timeout=10)  # TƒÉng timeout l√™n 10 gi√¢y
#             soup = BeautifulSoup(response.content, 'html.parser')
#             div_text = soup.find_all('div', class_='fulltext')

#             if not div_text:  # Ki·ªÉm tra n·∫øu div.fulltext kh√¥ng t·ªìn t·∫°i
#                 print(f"‚ö†Ô∏è ID {id}: Kh√¥ng t√¨m th·∫•y n·ªôi dung h·ª£p l·ªá.")
#                 break  # B·ªè qua ID n√†y

#             noidung_list = div_text[0].find_all('div')

#             if len(noidung_list) < 1:  # Ki·ªÉm tra n·∫øu kh√¥ng ƒë·ªß ph·∫ßn t·ª≠ trong div
#                 print(f"‚ö†Ô∏è ID {id}: D·ªØ li·ªáu kh√¥ng ƒë·∫ßy ƒë·ªß.")
#                 break  # B·ªè qua ID n√†y

#             noidung = noidung_list[1]
#             list_id.append(id)
#             list_noidung.append(str(noidung))
#             success = True  # ƒê√°nh d·∫•u l·∫•y d·ªØ li·ªáu th√†nh c√¥ng

#         except requests.exceptions.Timeout:
#             retry_count += 1
#             print(f"‚è≥ ID {id}: Timeout, th·ª≠ l·∫°i l·∫ßn {retry_count}/3...")
#             time.sleep(2)  # Ch·ªù 2 gi√¢y tr∆∞·ªõc khi th·ª≠ l·∫°i

#         except Exception as e:
#             print(f"‚ö†Ô∏è L·ªói khi l·∫•y ID {id}: {e}")
#             break  # B·ªè qua ID n√†y n·∫øu l·ªói kh√°c

#     # L∆∞u v√†o DB m·ªói 10 b·∫£n ghi ƒë·ªÉ tr√°nh m·∫•t d·ªØ li·ªáu
#     if len(list_id) % 10 == 0:
#         save_data(list_id, list_noidung)
#         list_id.clear()
#         list_noidung.clear()

# # L∆∞u nh·ªØng b·∫£n ghi c√≤n l·∫°i
# save_data(list_id, list_noidung)
# print("üéâ ƒê√£ ho√†n th√†nh c·∫≠p nh·∫≠t d·ªØ li·ªáu c√≤n thi·∫øu!")

